<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Local AI with Ollama &#8212; AI TOOLS COOKBOOK  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="NLP with Transformers" href="transformers.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="local-ai-with-ollama">
<span id="ollama-guide"></span><h1>Local AI with Ollama<a class="headerlink" href="#local-ai-with-ollama" title="Link to this heading">¶</a></h1>
<p><strong>Purpose</strong>: Run and customize open-weight language models locally.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h2>
<ol class="arabic">
<li><p>Install Ollama:
.. code-block:: bash</p>
<blockquote>
<div><p># Linux/macOS
curl -fsSL <a class="reference external" href="https://ollama.com/install.sh">https://ollama.com/install.sh</a> | sh</p>
<p># Windows (Powershell)
winget install ollama</p>
</div></blockquote>
</li>
<li><p>Verify installation:
.. code-block:: bash</p>
<blockquote>
<div><p>ollama –version</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="step-by-step-implementation">
<h2>Step-by-Step Implementation<a class="headerlink" href="#step-by-step-implementation" title="Link to this heading">¶</a></h2>
<section id="step-1-model-management">
<h3>Step 1: Model Management<a class="headerlink" href="#step-1-model-management" title="Link to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># List available models</span>
ollama<span class="w"> </span>list

<span class="c1"># Pull a model</span>
ollama<span class="w"> </span>pull<span class="w"> </span>mistral

<span class="c1"># Remove a model</span>
ollama<span class="w"> </span>rm<span class="w"> </span>mistral
</pre></div>
</div>
</section>
<section id="step-2-basic-python-integration">
<h3>Step 2: Basic Python Integration<a class="headerlink" href="#step-2-basic-python-integration" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">ollama</span>

<span class="c1"># Simple chat completion</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
        <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
        <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Explain quantum computing in simple terms&#39;</span>
    <span class="p">}]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="step-3-advanced-features">
<h3>Step 3: Advanced Features<a class="headerlink" href="#step-3-advanced-features" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stream responses</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama2&#39;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Tell me about Paris&#39;</span><span class="p">}],</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Custom model parameters</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;codellama&#39;</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s1">&#39;def factorial(n):&#39;</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s1">&#39;num_predict&#39;</span><span class="p">:</span> <span class="mi">128</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading">¶</a></h2>
<ul>
<li><p><strong>Model not found</strong>:
1. Verify model name:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>list
</pre></div>
</div>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>Pull the model first:
.. code-block:: bash</p>
<blockquote>
<div><p>ollama pull model_name</p>
</div></blockquote>
</li>
</ol>
</li>
<li><p><strong>Slow performance</strong>:
1. Use smaller models (e.g., mistral instead of llama2-70b)
2. Enable GPU acceleration:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">OLLAMA_NO_CUDA</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>ollama<span class="w"> </span>serve
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><strong>Memory issues</strong>:
1. Reduce context window:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;num_ctx&#39;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Link to this heading">¶</a></h2>
<ul>
<li><p><strong>Custom model creation</strong>:
1. Create Modelfile:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FROM mistral
SYSTEM &quot;&quot;&quot;You are a helpful AI assistant specialized in Python programming&quot;&quot;&quot;
TEMPLATE &quot;&quot;&quot;{{ .System }}

User: {{ .Prompt }}
Assistant: {{ .Response }}&quot;&quot;&quot;
</pre></div>
</div>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>Build and run:
.. code-block:: bash</p>
<blockquote>
<div><p>ollama create my-ai -f Modelfile
ollama run my-ai</p>
</div></blockquote>
</li>
</ol>
</li>
<li><p><strong>API endpoint</strong>:
.. code-block:: python</p>
<blockquote>
<div><p>import requests</p>
<dl>
<dt>response = requests.post(</dt><dd><p>‘<a class="reference external" href="http://localhost:11434/api/generate">http://localhost:11434/api/generate</a>’,
json={</p>
<blockquote>
<div><p>‘model’: ‘mistral’,
‘prompt’: ‘Explain blockchain’</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
</li>
</ul>
</section>
<section id="further-resources">
<h2>Further Resources<a class="headerlink" href="#further-resources" title="Link to this heading">¶</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/ollama/ollama">Ollama Documentation</a></p></li>
<li><p><a class="reference external" href="https://ollama.ai/library">Model Library</a></p></li>
<li><p><a class="reference external" href="https://github.com/ollama/ollama-python">Python API Reference</a></p></li>
</ul>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">AI TOOLS COOKBOOK</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Documentation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorflow_basics.html">Getting Started with TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_getting_started.html">Getting Started with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai_cloud_services.html">Using AI Cloud Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing for AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">Model Evaluation and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="AudioProcessing.html">Real-Time Audio Processing with sounddevice</a></li>
<li class="toctree-l1"><a class="reference internal" href="audio_file_handling.html">Audio File Handling with soundfile</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition.html">Speech Recognition with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">NLP with Transformers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Local AI with Ollama</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-by-step-implementation">Step-by-Step Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-usage">Advanced Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#further-resources">Further Resources</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="transformers.html" title="previous chapter">NLP with Transformers</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, ABHIN.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/ollama.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>